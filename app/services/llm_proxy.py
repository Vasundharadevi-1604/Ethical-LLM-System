def call_llm(prompt: str):
    """
    This is a placeholder for ANY LLM.
    Can be replaced with local or cloud models later.
    """
    return {
        "response": f"LLM response generated for: '{prompt}'"
    }
