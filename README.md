# ğŸ›¡ï¸ The Ethical LLM System

A production-ready Ethical Decision Layer for Large Language Models (LLMs) that evaluates user prompts for safety, ethics, and intent before generating responses.

This system demonstrates how responsible AI can be designed, enforced, and deployed in real-world applications.


---

ğŸŒ **Live Deployment:**  
https://ethical-llm-system.onrender.com


---

## ğŸš€ Project Overview

Large Language Models are powerful but can be misused through malicious, unsafe, or unethical prompts.  
**The Ethical LLM System** addresses this problem by introducing an **ethics-first decision layer** that evaluates user prompts *before* they reach the LLM.

The system:
- analyzes user intent,
- detects malicious or unsafe prompts,
- enforces ethical policies,
- generates safe alternatives when required,
- allows LLM responses only for approved prompts.

This project focuses on **Responsible AI, AI Safety, and Trust & Governance**.

---

## ğŸ¯ Key Objectives

- Prevent harmful prompt execution
- Enforce ethical AI usage at prompt level
- Provide transparent and explainable decisions
- Demonstrate a deployable Responsible AI system
- Bridge research-backed evaluation with real-world deployment

---

## ğŸ—ï¸ High-Level Architecture

```text
User Prompt
â”‚
â–¼
Ethical Decision Engine
â”‚
â”œâ”€â”€ Stage 1: Risk Detection
â”œâ”€â”€ Stage 2: Safety Classification
â”œâ”€â”€ Stage 3: Ethical Decision
â”‚
â”œâ”€â”€ SAFE â†’ LLM Response Generation
â””â”€â”€ UNSAFE â†’ Ethical Alternatives & Guidance
```

---

## âš™ï¸  System Components

### 1ï¸âƒ£ Streamlit Interface (`app.py`)
- Clean UI for user interaction
- Displays stage-wise ethical analysis
- Presents final ethical responses clearly

---

### 2ï¸âƒ£ Ethical Decision Layer (`ethical_layer/`)

| Module | Responsibility |
|------|---------------|
| `ethical_pipeline.py` | Orchestrates the complete ethical decision flow |
| `llm_generator.py` | Generates responses only for SAFE prompts |
| `ethical_alternatives.py` | Provides constructive guidance for UNSAFE prompts |

This layer is modular and can be **reused as an ethical middleware** in other AI systems.

---

### 3ï¸âƒ£ Research & Evaluation (`research/`)
- Malicious prompt experimentation
- Transformer-based classification models
- Ensemble evaluation for robustness

Models evaluated include:
- RoBERTa
- XLM-R
- SentiBERT

---

### 4ï¸âƒ£ Dataset (`data/`)
- Curated malicious prompt dataset
- Used to evaluate unsafe and adversarial inputs

---

## ğŸ“Š Research & Evaluation

The ethical decision logic is supported by:
- curated malicious prompt datasets,
- transformer-based classifiers,
- ensemble model evaluation.

Multiple models (RoBERTa, XLM-R, SentiBERT) were evaluated, and an ensemble approach was selected to improve robustness and reduce false positives.

ğŸ“„ Full evaluation and results are documented in the project report.

---

## ğŸ“ Project Structure

```text
Ethical-LLM-System/
â”‚
â”œâ”€â”€ app.py
â”‚
â”œâ”€â”€ ethical_layer/
â”‚   â”œâ”€â”€ ethical_pipeline.py
â”‚   â”œâ”€â”€ llm_generator.py
â”‚   â””â”€â”€ ethical_alternatives.py
â”‚
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ MaliciousQueries.csv
â”‚
â”œâ”€â”€ research/
â”‚   â”œâ”€â”€ run_experiment_final.py
â”‚   â””â”€â”€ bert_results_final.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ project_report.pdf
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Procfile
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Technologies Used

- Python
- Streamlit (UI & deployment)
- Gemini LLM (controlled generation)
- Transformers (BERT variants) for classification
- Render for public deployment

---

## ğŸŒ Deployment

- Publicly deployed using **Render**
- Fully accessible via web browser
- Demonstrates real-world AI system deployment

ğŸ”— **Live URL:** https://ethical-llm-system.onrender.com

---

## â–¶ï¸ Running Locally

```bash
git clone https://github.com/your-username/Ethical-LLM-System.git
cd Ethical-LLM-System

pip install -r requirements.txt
streamlit run app.py
