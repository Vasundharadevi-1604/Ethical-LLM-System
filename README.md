# ğŸ›¡ï¸ The Ethical LLM System

A **production-deployed Ethical Decision Layer for Large Language Models (LLMs)** that performs prompt-level risk analysis and enforces responsible AI behavior before any response is generated.

---

## ğŸ”— Live Demo

ğŸ‘‰ **Public Deployment:**  
https://ethical-llm-system.onrender.com

> A real-time Streamlit application demonstrating ethical prompt moderation and safe LLM response generation.

---

<h2>ğŸš€ Project Overview</h2>

Large Language Models are powerful but can be misused through malicious, unsafe, or unethical prompts.  
**The Ethical LLM System** addresses this problem by introducing an **ethics-first decision layer** that evaluates user prompts *before* they reach the LLM.

The system:
- analyzes user intent,
- detects malicious or unsafe prompts,
- enforces ethical policies,
- generates safe alternatives when required,
- allows LLM responses only for approved prompts.

This project focuses on **Responsible AI, AI Safety, and Trust & Governance**.

---

## ğŸ¯ Key Objectives

- Prevent harmful prompt execution
- Enforce ethical AI usage at prompt level
- Provide transparent and explainable decisions
- Demonstrate a deployable Responsible AI system
- Bridge research-backed evaluation with real-world deployment

---

## ğŸ§  System Architecture (High Level)


The architecture is designed to be:
- modular
- explainable
- deployment-friendly
- aligned with real-world Trust & Safety systems

ğŸ“„ Full details: `docs/architecture.md`

---

## âš™ï¸ Core Components

### 1. Streamlit Interface (`app.py`)
- Public-facing UI
- Accepts user prompts
- Displays stage-wise ethical analysis
- Shows final ethical response

### 2. Ethical Decision Engine (`ethics_engine/`)
- `ethical_pipeline.py` â€“ orchestrates ethical checks
- `llm_generator.py` â€“ generates responses for SAFE prompts
- `ethical_alternatives.py` â€“ handles UNSAFE prompts with safe guidance

This engine acts as the **ethical gatekeeper** of the system.

---

## ğŸ“Š Research & Evaluation

The ethical decision logic is supported by:
- curated malicious prompt datasets,
- transformer-based classifiers,
- ensemble model evaluation.

Multiple models (RoBERTa, XLM-R, SentiBERT) were evaluated, and an ensemble approach was selected to improve robustness and reduce false positives.

ğŸ“„ Full evaluation and results are documented in the project report.

---

## ğŸ“ Project Structure

```text
Ethical-LLM-System/
â”‚
â”œâ”€â”€ app.py
â”‚
â”œâ”€â”€ ethics_engine/
â”‚   â”œâ”€â”€ ethical_pipeline.py
â”‚   â”œâ”€â”€ llm_generator.py
â”‚   â””â”€â”€ ethical_alternatives.py
â”‚
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ MaliciousQueries.csv
â”‚
â”œâ”€â”€ research/
â”‚   â”œâ”€â”€ run_experiment_final.py
â”‚   â””â”€â”€ bert_results_final.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ project_report.pdf
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Procfile
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---

## ğŸ› ï¸ Technologies Used

- **Python**
- **Streamlit** (UI & deployment)
- **Gemini LLM** (controlled generation)
- **Transformers (BERT variants)** for classification
- **Render** for public deployment

---

## â–¶ï¸ Running Locally

```bash
git clone https://github.com/<your-username>/Ethical-LLM-System.git
cd Ethical-LLM-System

pip install -r requirements.txt
streamlit run app.py


```bash
git clone https://github.com/<your-username>/Ethical-LLM-System.git
cd Ethical-LLM-System

pip install -r requirements.txt
streamlit run app.py
